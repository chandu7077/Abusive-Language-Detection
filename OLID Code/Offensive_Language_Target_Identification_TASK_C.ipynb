{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Offensive_Language_Target_Identification_TASK_C",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "191zq3ZErihP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x #version 1.0\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import datetime\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "warnings.filterwarnings('ignore')\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY6ocyijAKdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TPU SET UP\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'NO TPU IS CONNECTED'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "#Google Cloud Access authentication\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "#creating session to run in TPU\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzwke0sxS6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMPORTING BERT PYTHON MODULES\n",
        "import sys\n",
        "!test -d bert_repo || git clone https://github.com/chandu7077/mybert.git bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w09zVYygSXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import python modules defined by BERT\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import run_classifier_with_tfhub\n",
        "import tokenization\n",
        "# import tfhub for loading the model\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYkaAlJNfhul",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#SETTING OUTPUT DIRECTORY AND TFHUB FOR LOADING PRETRAINED MODEL\n",
        "BUCKET = 'bertbase_lstm10'\n",
        "OUTPUT_DIR = 'gs://{}/{}'.format(\"olidtaskb\",BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('MODEL OUTPUT DIRECTORY : {0}'.format(OUTPUT_DIR))\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' \n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TylDOYd-6AH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bert tokenizer\n",
        "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJsd4KJfviyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMPORT DATASETS\n",
        "import pandas as pd\n",
        "train=pd.read_csv(\"olidtrainc.tsv\",sep=\"\\t\")\n",
        "test=pd.read_csv(\"olidtestc.tsv\",sep=\"\\t\")\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFTXP4z1QlSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OPTIONAL WE CAN USE / NOT USE\n",
        "!pip3 install emoji\n",
        "import emoji\n",
        "def remove_noise(tweet):\n",
        "    noises = ['URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m']\n",
        "    #noisesb = ['URL','\\'ve', 'n\\'t', '\\'s', '\\'m']\n",
        "    for noise in noises:\n",
        "      tweet=str(tweet)\n",
        "      tweet = tweet.replace(noise, '')\n",
        "    return tweet\n",
        "\n",
        "train.tweet=train.tweet.apply(lambda x:remove_noise(x))\n",
        "test.tweet=test.tweet.apply(lambda x:remove_noise(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqLScO8VvzYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample from bert run_classifier\n",
        "DATA_COLUMN = 'tweet'\n",
        "LABEL_COLUMN = 'label'\n",
        "label_list=[0,1,2]       # For task-c\n",
        "\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "train_examples = train.apply(lambda x: InputExample(guid=None,\n",
        "                                                  text_a = x[DATA_COLUMN], \n",
        "                                                  text_b = None, \n",
        "                                                  label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_examples = test.apply(lambda x: InputExample(guid=None,\n",
        "                                                  text_a = x[DATA_COLUMN], \n",
        "                                                  text_b = None, \n",
        "                                                label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU21-HLOVKNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NEEDED FOR BERT SEQUENCE OUTPUT WITH ATTENTION\n",
        "class Attention(tf.keras.Model):\n",
        "\tdef __init__(self, units):\n",
        "\t\tsuper(Attention, self).__init__()\n",
        "\t#trainable weights \n",
        "\t\tself.W1 = tf.keras.layers.Dense(units)\n",
        "\t\tself.W2 = tf.keras.layers.Dense(units)\n",
        "\t\tself.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "\tdef call(self, features, hidden):\n",
        "\t\t#hidden shape == (batch_size, hidden size)\n",
        "\t\thidden_with_time_axis = tf.expand_dims(hidden, 1) #(8,1,768)\n",
        "\t\t  \n",
        "\t\t#score shape == (batch_size, max_length, 1)\n",
        "\t\tscore = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) #(8,20,1)\n",
        "\n",
        "\t\t#attention_weights shape == (batch_size, max_length, 1)\n",
        "\t\tattention_weights = tf.nn.softmax(self.V(score), axis=1)      #(8,20,1)\n",
        "\n",
        "\t\t#context_vector shape after sum == (batch_size, hidden_size\n",
        "\t\tcontext_vector = attention_weights * features                #(8,20,768)\n",
        "\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)       #(8,768)\n",
        "\t\treturn context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4GQEzy4g0pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_attention_wce_model(is_training, input_ids, input_mask, segment_ids, labels,num_labels, bert_hub_module_handle):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  #loading weights from hub\n",
        "  bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True) \n",
        "\n",
        "  #structuring inputs\n",
        "  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids) \n",
        "\n",
        "  #giving inputs to BERT layers\n",
        "  bert_outputs = bert_module(inputs=bert_inputs,signature=\"tokens\",as_dict=True)\n",
        "\n",
        "  #getting sequence output from the BERT  (batchsize,128,768)\n",
        "  output_layer=bert_outputs[\"sequence_output\"]\n",
        "\n",
        "  #hidden_size=768\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  #setup softmax weights and biases\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    #token embeddings of first 20 tokens \n",
        "    t1=output_layer[:,1:21]\n",
        "\n",
        "    #token embedding of [CLS] token\n",
        "    t2=output_layer[:,0]\n",
        "\n",
        "    #setting up LSTM cell unit \n",
        "    lstmcell =  tf.nn.rnn_cell.LSTMCell(768, state_is_tuple=True)\n",
        "\n",
        "    #setting sequence of LSTMS with 20 token embeddings as input \n",
        "    outputs, states = tf.nn.dynamic_rnn(lstmcell, t1, sequence_length=[20]*t1.shape[0].value, dtype=tf.float32)\n",
        "\n",
        "    #output from attention layer \n",
        "    context_vector, attention_weights = Attention(1)(output_layer[:,1:21], states.h)\n",
        "\n",
        "    #mean of context vector and [CLS] token \n",
        "    output_layer=tf.reduce_mean([t2,context_vector],0)\n",
        "    \n",
        "\n",
        "    if is_training:\n",
        "      #using dropout for regularisation : dropout rate=0.9\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    # computing logits W.X + b  \n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "    #calculating softmax probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    #convert true labels to one hot \n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    # deduce weights for batch samples based on their true label\n",
        "    weights = tf.reduce_sum(class_weights * one_hot_labels, axis=1)\n",
        "\n",
        "    # compute your (unweighted) softmax cross entropy loss\n",
        "    unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits)\n",
        "\n",
        "    # apply the weights\n",
        "    per_example_loss = unweighted_losses * weights\n",
        "\n",
        "    # reduce the result to get your final loss\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOPT-l5j6q7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lstm_wce_model(is_training, input_ids, input_mask, segment_ids, labels,num_labels, bert_hub_module_handle):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  #loading weights from hub\n",
        "  bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True) \n",
        "\n",
        "  #structuring inputs\n",
        "  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids) \n",
        "\n",
        "  #giving inputs to BERT layers\n",
        "  bert_outputs = bert_module(inputs=bert_inputs,signature=\"tokens\",as_dict=True)\n",
        "\n",
        "  #getting sequence output from the BERT  (batchsize,128,768)\n",
        "  output_layer=bert_outputs[\"sequence_output\"]\n",
        "\n",
        "  #hidden_size=768\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  #setup softmax weights and biases\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    #token embeddings of first 20 tokens \n",
        "    t1=output_layer[:,1:21]\n",
        "\n",
        "    #token embedding of [CLS] token\n",
        "    t2=output_layer[:,0]\n",
        "\n",
        "    #setting up LSTM cell unit \n",
        "    lstmcell =  tf.nn.rnn_cell.LSTMCell(768, state_is_tuple=True)\n",
        "\n",
        "    #setting sequence of LSTMS with 20 token embeddings as input \n",
        "    outputs, states = tf.nn.dynamic_rnn(lstmcell, t1, sequence_length=[20]*t1.shape[0].value, dtype=tf.float32)\n",
        "\n",
        "    #mean of last lstm cell state vector and [CLS] token \n",
        "    output_layer=tf.reduce_mean([t2,states.h],0)\n",
        "    \n",
        "    if is_training:\n",
        "      #using dropout for regularisation : dropout rate=0.9\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    # computing logits W.X + b  \n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "    #calculating softmax probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    #convert true labels to one hot \n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    # deduce weights for batch samples based on their true label\n",
        "    weights = tf.reduce_sum(class_weights * one_hot_labels, axis=1)\n",
        "\n",
        "    # compute your (unweighted) softmax cross entropy loss\n",
        "    unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits)\n",
        "\n",
        "    # apply the weights\n",
        "    per_example_loss = unweighted_losses * weights\n",
        "\n",
        "    # reduce the result to get your final loss\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6qNI9HZ_Rsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_simple_model(is_training, input_ids, input_mask, segment_ids, labels,num_labels, bert_hub_module_handle):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  #loading weights from hub\n",
        "  bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True) \n",
        "\n",
        "  #structuring inputs\n",
        "  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids) \n",
        "\n",
        "  #giving inputs to BERT layers\n",
        "  bert_outputs = bert_module(inputs=bert_inputs,signature=\"tokens\",as_dict=True)\n",
        "\n",
        "  #getting pooled output from the BERT  (batchsize,768)\n",
        "  output_layer=bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  #hidden_size=768\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  #setup softmax weights and biases\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    \n",
        "    if is_training:\n",
        "      #using dropout for regularisation : dropout rate=0.9\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    # computing logits W.X + b  \n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "    #calculating softmax probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    #convert true labels to one hot \n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    # deduce weights for batch samples based on their true label\n",
        "    weights = tf.reduce_sum(class_weights * one_hot_labels, axis=1)\n",
        "\n",
        "    # compute your (unweighted) softmax cross entropy loss\n",
        "    unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits)\n",
        "\n",
        "    # apply the weights\n",
        "    per_example_loss = unweighted_losses * weights\n",
        "\n",
        "    # reduce the result to get your final loss\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzLKRCzWOImU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MODEL FUNCTION BUILDER\n",
        "train_hook_list= []\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,num_warmup_steps, use_tpu, bert_hub_module_handle,model_type=\"lstm\"):\n",
        "  def model_fn(features, labels, mode, params):\n",
        "    for name in sorted(features.keys()):\n",
        "\n",
        "    # input ids which are obtained from bert tokenizer\n",
        "    input_ids = features[\"input_ids\"]\n",
        "\n",
        "    # input masks to represent the actual sentence (1's along length of sentence , 0's for padding) and padding sequence\n",
        "    input_mask = features[\"input_mask\"]\n",
        "     \n",
        "    # segment ids to distinguish in case of two sentence tasks (not needed for this task)\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "\n",
        "    # label for sentence \n",
        "    label_ids = features[\"label_ids\"]\n",
        "     \n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    \n",
        "    if model_type==\"lstm\":\n",
        "      (total_loss, per_example_loss, logits, probabilities) = create_lstm_wce_model(is_training,\n",
        "      input_ids, input_mask, segment_ids, label_ids, num_labels,bert_hub_module_handle)\n",
        "\n",
        "    elif model_type==\"attention\":\n",
        "      (total_loss, per_example_loss, logits, probabilities) = create_attention_wce_model(is_training,\n",
        "      input_ids, input_mask, segment_ids, label_ids, num_labels,bert_hub_module_handle)\n",
        "\n",
        "    elif model_type==\"simple\":\n",
        "      (total_loss, per_example_loss, logits, probabilities) = create_simple_model(is_training,\n",
        "      input_ids, input_mask, segment_ids, label_ids, num_labels,bert_hub_module_handle)\n",
        "    \n",
        "    #output specifications\n",
        "    output_spec = None\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      train_op = optimization.create_optimizer(total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "      \n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode,loss=total_loss,train_op=train_op)\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits):\n",
        "        #classification metrics\n",
        "        predicted_labels = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        loss = tf.metrics.mean(per_example_loss)\n",
        "        out={\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"loss\":loss\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode,loss=total_loss,eval_metrics=eval_metrics)\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions={\"probabilities\": probabilities})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYVYULZiKvUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 8\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS =3.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "#increasing learning rate gradually\n",
        "WARMUP_PROPORTION = 0.1\n",
        "SAVE_CHECKPOINTS_STEPS = 10000 \n",
        "SAVE_SUMMARY_STEPS = 1000\n",
        "\n",
        "num_train_steps = int(len(train) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "#TPU CLUSTER SETUP FOR 8 TPU CORES\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "print(\"no of train steps:\",num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwcsdbLuIX2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make tensorflow hub write to gcs bucket\n",
        "import os\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "\n",
        "label_list=[0,1,2]\n",
        "\n",
        "#setup model type\n",
        "model_type=\"attention\"\n",
        "\n",
        "#setup modelfunction ==> create model, evaluation metrics, predictions\n",
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  bert_hub_module_handle=BERT_MODEL_HUB,\n",
        "  model_type=model_type\n",
        ")\n",
        "\n",
        "estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U_c8s2AvhgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the model\n",
        "def oli_train():\n",
        "  #convert examples to features in the BERT format\n",
        "  train_features = run_classifier.convert_examples_to_features(train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator_from_tfhub.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "oli_train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIWX5JTSEfjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing the model\n",
        "import numpy as np\n",
        "\n",
        "def oli_test(test):\n",
        "  test_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in test[\"tweet\"]]\n",
        "  test_features = run_classifier.convert_examples_to_features(test_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=test_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  predictions = estimator_from_tfhub.predict(predict_input_fn)\n",
        "predictions=oli_test(test)\n",
        "\n",
        "\n",
        "def test_predictions():\n",
        "  ans5=[]\n",
        "  for it in predictions:\n",
        "    ans5.append(it)\n",
        "  pred=[]\n",
        "  probs=[]\n",
        "  for s,v in zip(test[\"tweet\"],ans5):\n",
        "    pro=v[\"probabilities\"]\n",
        "    v=np.argmax(pro)\n",
        "    pred.append(v)\n",
        "    probs.append(pro[1])\n",
        "  return pred,probs\n",
        "\n",
        "prediction_labels,pos_probabilities=test_predictions()\n",
        "true_labels=list(test.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB3RFd3p97Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(true_labels,prediction_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7GUDgt6aqaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#classification report\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(truel,pred,target_names=[\"IND\",\"GRP\",\"OTH\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwZF207R3equ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUC AND PRECISION RECALL CURVE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import auc,roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "\n",
        "lr_probs = pos_probabilities\n",
        "lr_precision, lr_recall, _ = precision_recall_curve(truel, lr_probs)\n",
        "lr_f1, lr_auc = f1_score(truel, pred), auc(lr_recall, lr_precision,)\n",
        "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "c=0\n",
        "for i in truel:\n",
        "  if i==0:\n",
        "    c+=1\n",
        "no_skill = c / len(truel)\n",
        "pyplot.plot([1, 0], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "pyplot.plot(lr_recall, lr_precision, marker='.', label='BERT LARGE')\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "pyplot.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
